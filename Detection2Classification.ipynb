{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './yolov5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from yolov5.utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from yolov5.utils.plots import Annotator, colors, save_one_box\n",
    "from yolov5.utils.augmentations import letterbox\n",
    "from yolov5.utils.torch_utils import select_device, time_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import copy\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import logging\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from easydict import EasyDict as edict\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = edict({\n",
    "    \"yolo_model\" : \"./weights/car_best.pt\",\n",
    "    \"output\" : \"inference/output\",\n",
    "    \"conf_thres\" : 0.1,\n",
    "    \"iou_thres\" : 0.3,\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"save_txt\" : True,\n",
    "    \"save_vid\" : False,\n",
    "    \"show_vid\" : False,\n",
    "    \"classes\" : [0],\n",
    "    \"agnostic_nms\" : True,\n",
    "    \"augment\" : False,\n",
    "    \"evaluate\" : True,\n",
    "    \"half\" : True,\n",
    "    \"visualize\" : True,\n",
    "    \"max_det\" : 1000,\n",
    "    \"dnn\" : True,\n",
    "    \"project\" : \"runs/track\",\n",
    "    \"exist_ok\" : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.1-32-gc13d4ce torch 1.10.1+cu113 CUDA:0 (NVIDIA RTX A6000, 48685MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 574 layers, 139970872 parameters, 0 gradients, 208.1 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "device = select_device(opt.device)\n",
    "model = DetectMultiBackend(opt.yolo_model, device=device, dnn=opt.dnn, fp16=False)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(640, s=stride)  # check image size\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgv1_list = sorted(glob(\"/data/IEEE_BigData/test/test_v1/*.jpg\"))\n",
    "imgv2_list = sorted(glob(\"/data/IEEE_BigData/test/test_v2/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgv1_list[0].split(\"/\")[-1].replace(\"jpg\", \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"0\" : \"car\",\n",
    "       \"1\" : \"truck\",\n",
    "       \"2\" : \"motorcycle\",\n",
    "       \"3\" : \"bicycle\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"./submission/0913_only_car_v1.txt\" ,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [03:07<00:00,  8.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(imgv1_list))):\n",
    "    img = cv2.imread(imgv1_list[i])\n",
    "    shape = img.shape\n",
    "    img = letterbox(img, imgsz, stride, auto=pt)[0]\n",
    "    img = img.transpose((2, 0, 1))[::-1]\n",
    "    img = np.ascontiguousarray(img)\n",
    "    \n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.to(device)\n",
    "    img = img.float()\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred = model(img, augment=True, visualize=False)\n",
    "    det = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, opt.classes, opt.agnostic_nms, max_det=opt.max_det)[0]\n",
    "    \n",
    "    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], shape)\n",
    "    \n",
    "    # Print results\n",
    "    for c in det[:, -1].unique():\n",
    "        n = (det[:, -1] == c).sum()  # detections per class\n",
    "\n",
    "    xyxys = det[:, 0:4]\n",
    "    confs = det[:, 4]\n",
    "    clss = det[:, 5]\n",
    "    \n",
    "    f_name = imgv1_list[i].split(\"/\")[-1].replace(\"jpg\", \"txt\")\n",
    "    \n",
    "    sentence = f\"{f_name}\"\n",
    "    \n",
    "    for j in range(len(xyxys)):\n",
    "        sentence = sentence +  f\" {dic[str(int(clss[j]))]} {confs[j]} {xyxys[j][0]} {xyxys[j][1]} {xyxys[j][2]} {xyxys[j][3]}\"\n",
    "        \n",
    "    sentence = sentence + \"\\n\"\n",
    "    \n",
    "    txt.write(sentence)\n",
    "    \n",
    "txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"./submission/0913_only_car_v2.txt\" ,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [01:40<00:00, 14.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(imgv2_list))):\n",
    "    img = cv2.imread(imgv2_list[i])\n",
    "    shape = img.shape\n",
    "    img = letterbox(img, imgsz, stride, auto=pt)[0]\n",
    "    img = img.transpose((2, 0, 1))[::-1]\n",
    "    img = np.ascontiguousarray(img)\n",
    "    \n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.to(device)\n",
    "    img = img.float()\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred = model(img, augment=False, visualize=False)\n",
    "    det = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, opt.classes, opt.agnostic_nms, max_det=opt.max_det)[0]\n",
    "    \n",
    "    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], shape)\n",
    "    \n",
    "    # Print results\n",
    "    for c in det[:, -1].unique():\n",
    "        n = (det[:, -1] == c).sum()  # detections per class\n",
    "\n",
    "    xyxys = det[:, 0:4]\n",
    "    confs = det[:, 4]\n",
    "    clss = det[:, 5]\n",
    "    \n",
    "    f_name = imgv2_list[i].split(\"/\")[-1].replace(\"jpg\", \"txt\")\n",
    "    \n",
    "    sentence = f\"{f_name}\"\n",
    "    \n",
    "    for j in range(len(xyxys)):\n",
    "        sentence = sentence +  f\" {dic[str(int(clss[j]))]} {confs[j]} {xyxys[j][0]} {xyxys[j][1]} {xyxys[j][2]} {xyxys[j][3]}\"\n",
    "        \n",
    "    sentence = sentence + \"\\n\"\n",
    "    \n",
    "    txt.write(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
